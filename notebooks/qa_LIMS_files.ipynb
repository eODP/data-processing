{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA LIMS files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some basic QA on the LIMS files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts/')\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from normalize_data import (\n",
    "    check_duplicate_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMS_lith_paths = [\n",
    "    'cleaned_data/Lithology_CSV',\n",
    "]\n",
    "\n",
    "LIMS_paleo_paths = [\n",
    "    'cleaned_data/Micropal_CSV_1', \n",
    "    'cleaned_data/Micropal_CSV_2',\n",
    "    'cleaned_data/Micropal_CSV_3',\n",
    "    'cleaned_data/Micropal_CSV_revised',\n",
    "]\n",
    "\n",
    "LIMS_paths = LIMS_lith_paths + LIMS_paleo_paths\n",
    "\n",
    "taxa_list = 'cleaned_data/taxa/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## duplicate column names\n",
    "\n",
    "check if csv has duplicate column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_columns(directories, file_extension='csv'):\n",
    "    for directory in directories:\n",
    "        raw_csvs = glob.glob(f\"{directory}/**/*.{file_extension}\", recursive=True)\n",
    "\n",
    "        for path in raw_csvs:\n",
    "            content = pd.read_csv(path, nrows=1)\n",
    "            content.dropna(inplace=True, axis='columns', how='all')\n",
    "\n",
    "            check_duplicate_columns(content, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIMS: Leg 317 - present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_data/Lithology_CSV/323 Core Description Template_U1341A.csv, GRAVEL SIZE CLAST: duplicate columns have different values\n"
     ]
    }
   ],
   "source": [
    "duplicate_columns(LIMS_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Janus: Leg 130 - 312"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOAA: Leg 1 - 129"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for files that contain taxon name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/eODP/data-processing/blob/master/notebooks/cleaned_data/Micropal_CSV_2/374_U1522A_palynology.csv\n",
      "https://github.com/eODP/data-processing/blob/master/notebooks/cleaned_data/Micropal_CSV_2/374_U1523E_palynology.csv\n",
      "https://github.com/eODP/data-processing/blob/master/notebooks/cleaned_data/Micropal_CSV_2/374_U1525A_palynology.csv\n",
      "https://github.com/eODP/data-processing/blob/master/notebooks/cleaned_data/Micropal_CSV_2/374_U1523B_palynology.csv\n",
      "https://github.com/eODP/data-processing/blob/master/notebooks/cleaned_data/Micropal_CSV_2/374_U1524C_palynology.csv\n",
      "https://github.com/eODP/data-processing/blob/master/notebooks/cleaned_data/Micropal_CSV_2/374_U1524A_palynology.csv\n",
      "https://github.com/eODP/data-processing/blob/master/notebooks/cleaned_data/Micropal_CSV_2/374_U1523A_palynology.csv\n",
      "https://github.com/eODP/data-processing/blob/master/notebooks/cleaned_data/Micropal_CSV_2/374_U1521A_palynology.csv\n"
     ]
    }
   ],
   "source": [
    "taxon_name =  'Preservation palynofacies'\n",
    "\n",
    "for clean_data_path in LIMS_paleo_paths:\n",
    "    raw_csvs = glob.glob(f\"{clean_data_path}/*.csv\")\n",
    "\n",
    "    for path in raw_csvs:\n",
    "        df = pd.read_csv(path, nrows=1)\n",
    "        df.dropna(how=\"all\", axis=\"columns\")\n",
    "        if taxon_name in list(df.columns):\n",
    "            url = 'https://github.com/eODP/data-processing/blob/master/notebooks/'\n",
    "            print(f'{url}{path}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for duplicate samples in all mircopal files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =[]\n",
    "for clean_data_path in LIMS_paleo_paths:\n",
    "    raw_csvs = glob.glob(f\"{clean_data_path}/*.csv\")\n",
    "    \n",
    "    for path in raw_csvs:\n",
    "        content = pd.read_csv(path, usecols=['Sample'])\n",
    "        content.dropna(inplace=True, axis='index', how='all')\n",
    "        new_df = content[content.duplicated()]\n",
    "        for index, row in new_df.iterrows():\n",
    "            data.append({'sample': row['Sample'], 'path': path})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "610"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## files with duplicate rows\n",
    "\n",
    "Files that have identical rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_rows =[]\n",
    "files_dup_rows = set()\n",
    "\n",
    "for clean_data_path in LIMS_paths:\n",
    "    raw_csvs = glob.glob(f\"{clean_data_path}/*.csv\")\n",
    "    \n",
    "    for path in raw_csvs:\n",
    "        content = pd.read_csv(path)\n",
    "        content.dropna(inplace=True, axis='index', how='all')\n",
    "        new_df = content[content.duplicated()]\n",
    "        \n",
    "        for index, row in new_df.iterrows():\n",
    "            dup_rows.append({'sample': row['Sample'], 'path': path})\n",
    "            files_dup_rows.add(path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dup_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files_dup_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(dup_rows)\n",
    "new_df.to_csv('tmp/csvs_with_duplicate_rows.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing samples names\n",
    "\n",
    "Look for files that have rows with no sample names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_samples = set()\n",
    "url = \"https://github.com/eODP/data-processing/blob/master/notebooks/\"\n",
    "\n",
    "for clean_data_path in LIMS_paths:\n",
    "    raw_csvs = glob.glob(f\"{clean_data_path}/*.csv\")\n",
    "\n",
    "    for path in raw_csvs:\n",
    "        content = pd.read_csv(path)\n",
    "        content.dropna(inplace=True, axis='index', how='all')\n",
    "        \n",
    "        if sum(content.isna()['Sample']) > 0:  \n",
    "            missing_samples.add(url + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"files with missing sample names\": list(missing_samples)})\n",
    "df.to_csv('tmp/csvs_with_missing_samples.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
