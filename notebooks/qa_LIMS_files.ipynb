{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA LIMS files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some basic QA on the LIMS files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts/')\n",
    "import glob\n",
    "import shutil\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from normalize_data import (\n",
    "    check_duplicate_columns,\n",
    "    extract_taxon_group_from_filename,\n",
    "    csv_cleanup,\n",
    "    create_sample_name_for_row\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMS_lith_paths = [\n",
    "    'cleaned_data/Lithology_CSV',\n",
    "]\n",
    "\n",
    "LIMS_paleo_paths = [\n",
    "    'cleaned_data/Micropal_CSV_1', \n",
    "    'cleaned_data/Micropal_CSV_2',\n",
    "    'cleaned_data/Micropal_CSV_3',\n",
    "    'cleaned_data/Micropal_CSV_revised',\n",
    "]\n",
    "\n",
    "LIMS_paths = LIMS_lith_paths + LIMS_paleo_paths\n",
    "\n",
    "taxa_list = 'cleaned_data/taxa/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## duplicate column names\n",
    "\n",
    "check if csv has duplicate column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_columns(directories, file_extension='csv'):\n",
    "    for directory in directories:\n",
    "        raw_csvs = glob.glob(f\"{directory}/**/*.{file_extension}\", recursive=True)\n",
    "\n",
    "        for path in raw_csvs:\n",
    "            content = pd.read_csv(path)\n",
    "            content.dropna(inplace=True, axis='columns', how='all')\n",
    "\n",
    "            check_duplicate_columns(content, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='cleaned_data/Micropal_CSV_3/341_benthic_forams_U1417B.csv'\n",
    "content = pd.read_csv(path)\n",
    "content.dropna(inplace=True, axis='columns', how='all')\n",
    "\n",
    "check_duplicate_columns(content, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIMS: Leg 317 - present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_data/Lithology_CSV/323 Core Description Template_U1341A.csv, GRAVEL SIZE CLAST: duplicate columns have different values\n"
     ]
    }
   ],
   "source": [
    "duplicate_columns(LIMS_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Janus: Leg 130 - 312"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOAA: Leg 1 - 129"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for files that contain taxon name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/eODP/data-processing/blob/master/notebooks/cleaned_data/Micropal_CSV_1/320_U1334A_Nannofossils_1.csv\n",
      "https://github.com/eODP/data-processing/blob/master/notebooks/cleaned_data/Micropal_CSV_1/320_U1335A_Nannofossils_1.csv\n",
      "https://github.com/eODP/data-processing/blob/master/notebooks/cleaned_data/Micropal_CSV_1/320_U1336A_Nannofossils_1.csv\n",
      "https://github.com/eODP/data-processing/blob/master/notebooks/cleaned_data/Micropal_CSV_1/320_U1335B_Nannofossils_1.csv\n",
      "https://github.com/eODP/data-processing/blob/master/notebooks/cleaned_data/Micropal_CSV_1/320_U1334B_Nannofossils_1.csv\n",
      "https://github.com/eODP/data-processing/blob/master/notebooks/cleaned_data/Micropal_CSV_1/320_U1336B_Nannofossils.csv\n",
      "https://github.com/eODP/data-processing/blob/master/notebooks/cleaned_data/Micropal_CSV_1/320_U1334C_Nannofossils_1.csv\n",
      "https://github.com/eODP/data-processing/blob/master/notebooks/cleaned_data/Micropal_CSV_3/321_U1338B_Planktic_forams.csv\n",
      "https://github.com/eODP/data-processing/blob/master/notebooks/cleaned_data/Micropal_CSV_3/321_U1337A_Planktic_forams.csv\n",
      "https://github.com/eODP/data-processing/blob/master/notebooks/cleaned_data/Micropal_CSV_3/321_U1338A_Planktic_forams.csv\n"
     ]
    }
   ],
   "source": [
    "taxon_name =  'Preservation palynofacies'\n",
    "\n",
    "for clean_data_path in LIMS_paleo_paths:\n",
    "    raw_csvs = glob.glob(f\"{clean_data_path}/*.csv\")\n",
    "\n",
    "    for path in raw_csvs:\n",
    "        df = pd.read_csv(path, nrows=1)\n",
    "        df.dropna(how=\"all\", axis=\"columns\")\n",
    "        if taxon_name in list(df.columns):\n",
    "            url = 'https://github.com/eODP/data-processing/blob/master/notebooks/'\n",
    "            print(f'{url}{path}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for duplicate samples in all mircopal files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =[]\n",
    "for clean_data_path in LIMS_paleo_paths:\n",
    "    raw_csvs = glob.glob(f\"{clean_data_path}/*.csv\")\n",
    "    \n",
    "    for path in raw_csvs:\n",
    "        content = pd.read_csv(path, usecols=['Sample'])\n",
    "        content.dropna(inplace=True, axis='index', how='all')\n",
    "        new_df = content[content.duplicated()]\n",
    "        for index, row in new_df.iterrows():\n",
    "            data.append({'sample': row['Sample'], 'path': path})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "610"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## files with duplicate rows\n",
    "\n",
    "Files that have identical rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_rows =[]\n",
    "files_dup_rows = set()\n",
    "\n",
    "for clean_data_path in LIMS_paths:\n",
    "    raw_csvs = glob.glob(f\"{clean_data_path}/*.csv\")\n",
    "    \n",
    "    for path in raw_csvs:\n",
    "        content = pd.read_csv(path)\n",
    "        content.dropna(inplace=True, axis='index', how='all')\n",
    "        new_df = content[content.duplicated()]\n",
    "        \n",
    "        for index, row in new_df.iterrows():\n",
    "            dup_rows.append({'sample': row['Sample'], 'path': path})\n",
    "            files_dup_rows.add(path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dup_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files_dup_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(dup_rows)\n",
    "new_df.to_csv('tmp/csvs_with_duplicate_rows.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing samples names\n",
    "\n",
    "Look for files that have rows with no sample names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_samples = set()\n",
    "\n",
    "for clean_data_path in LIMS_paths:\n",
    "    raw_csvs = glob.glob(f\"{clean_data_path}/*.csv\")\n",
    "\n",
    "    for path in raw_csvs:\n",
    "        content = pd.read_csv(path)\n",
    "        content.dropna(inplace=True, axis='index', how='all')\n",
    "        \n",
    "        if sum(content.isna()['Sample']) > 0:  \n",
    "            missing_samples.add(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"path\": list(missing_samples)})\n",
    "df.to_csv('tmp/csvs_with_missing_samples.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare sample name with exp...extra columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temp_sample_name(df):\n",
    "    \"\"\"Uses Exp...A/W columns to create a name for a sample\"\"\"\n",
    "    names = {\"Exp\", \"Site\", \"Hole\", \"Core\", \"Type\", \"Section\", \"A/W\"}\n",
    "    if names.issubset(df.columns):\n",
    "        df[\"temp_sample\"] = df.apply(\n",
    "            lambda row: create_sample_name_for_row(row, df.columns), axis=1\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"File does not have the expected columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Sample Top [cm] Bottom [cm] Top Depth [m]  \\\n",
      "0  375-U1518F-2R-CC-PAL-FORAM        0          10         199.8   \n",
      "1  375-U1518F-3R-CC-PAL-FORAM        0          10        212.49   \n",
      "2  375-U1518F-4R-CC-PAL-FORAM        0          10        219.21   \n",
      "3  375-U1518F-5R-CC-PAL-FORAM        0          10        229.91   \n",
      "4  375-U1518F-6R-CC-PAL-FORAM        0          13        239.08   \n",
      "\n",
      "  Bottom Depth [m] Preservation Group abundance General comment  \\\n",
      "0            199.9           VG               D             NaN   \n",
      "1           212.59           VG               D             NaN   \n",
      "2           219.31       VG (M)               A             NaN   \n",
      "3           230.01           VG               A             NaN   \n",
      "4           239.21           VG               D             NaN   \n",
      "\n",
      "  Beella digitata Beella praedigitata  ... Hole Core Type Section  A/W  \\\n",
      "0             NaN                 NaN  ...    F    2    R      CC  PAL   \n",
      "1             NaN                 NaN  ...    F    3    R      CC  PAL   \n",
      "2             NaN                 NaN  ...    F    4    R      CC  PAL   \n",
      "3             NaN                 NaN  ...    F    5    R      CC  PAL   \n",
      "4             NaN                 NaN  ...    F    6    R      CC  PAL   \n",
      "\n",
      "  Extra Sample ID Data Zone name (short) Zone name  \\\n",
      "0                FORAM               NaN       NaN   \n",
      "1                FORAM               NaN       NaN   \n",
      "2                FORAM               NaN       NaN   \n",
      "3                FORAM               NaN       NaN   \n",
      "4                FORAM               NaN       NaN   \n",
      "\n",
      "                  temp_sample valid_sample_name  \n",
      "0  375-U1518F-2R-CC-PAL-FORAM              True  \n",
      "1  375-U1518F-3R-CC-PAL-FORAM              True  \n",
      "2  375-U1518F-4R-CC-PAL-FORAM              True  \n",
      "3  375-U1518F-5R-CC-PAL-FORAM              True  \n",
      "4  375-U1518F-6R-CC-PAL-FORAM              True  \n",
      "\n",
      "[5 rows x 136 columns]\n"
     ]
    }
   ],
   "source": [
    "path = 'cleaned_data/Micropal_CSV_1/375_U1518F_planktic_forams.csv'\n",
    "content = pd.read_csv(path, dtype=str)\n",
    "\n",
    "create_temp_sample_name(content)\n",
    "\n",
    "content['valid_sample_name'] = content['Sample'] == content['temp_sample']\n",
    "\n",
    "if sum(content['valid_sample_name']) > 0:\n",
    "    print(content.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "363-U1482A-1H-CC-nan\n",
      "----\n",
      "363-U1482A-2H-CC-nan\n",
      "----\n",
      "363-U1482A-3H-CC-nan\n",
      "----\n",
      "363-U1482A-4H-CC-nan\n",
      "----\n",
      "363-U1482A-5H-CC-nan\n",
      "----\n",
      "363-U1482A-6H-CC-nan\n",
      "----\n",
      "363-U1482A-7H-CC-nan\n",
      "----\n",
      "363-U1482A-8H-CC-nan\n",
      "----\n",
      "363-U1482A-9H-CC-nan\n",
      "----\n",
      "363-U1482A-10H-CC-nan\n",
      "----\n",
      "363-U1482A-11H-CC-nan\n",
      "----\n",
      "363-U1482A-12H-CC-nan\n",
      "----\n",
      "363-U1482A-13H-CC-nan\n",
      "----\n",
      "363-U1482A-14H-CC-nan\n",
      "----\n",
      "363-U1482A-15H-CC-nan\n",
      "----\n",
      "363-U1482A-16H-CC-nan\n",
      "----\n",
      "363-U1482A-17H-CC-nan\n",
      "----\n",
      "363-U1482A-18H-CC-nan\n",
      "----\n",
      "363-U1482A-19H-CC-nan\n",
      "----\n",
      "363-U1482A-20H-CC-nan\n",
      "----\n",
      "363-U1482A-21H-CC-nan\n",
      "----\n",
      "363-U1482A-22H-CC-nan\n",
      "----\n",
      "363-U1482A-23H-CC-nan\n",
      "----\n",
      "363-U1482A-24H-CC-nan\n",
      "----\n",
      "363-U1482A-25H-CC-nan\n",
      "----\n",
      "363-U1482A-26H-CC-nan\n",
      "----\n",
      "363-U1482A-27H-CC-nan\n",
      "----\n",
      "363-U1482A-28H-CC-nan\n",
      "----\n",
      "363-U1482A-29H-CC-nan\n",
      "----\n",
      "363-U1482A-30H-CC-nan\n",
      "----\n",
      "363-U1482A-31H-CC-nan\n",
      "----\n",
      "363-U1482A-32H-CC-nan\n",
      "----\n",
      "363-U1482A-33H-CC-nan\n",
      "----\n",
      "363-U1482A-34H-CC-nan\n",
      "----\n",
      "363-U1482A-35H-CC-nan\n",
      "----\n",
      "363-U1482A-36H-CC-nan\n",
      "----\n",
      "363-U1482A-37H-CC-nan\n",
      "----\n",
      "363-U1482A-39F-CC-nan\n",
      "----\n",
      "363-U1482A-40F-CC-nan\n",
      "----\n",
      "363-U1482A-41F-CC-nan\n",
      "----\n",
      "363-U1482A-42F-CC-nan\n",
      "----\n",
      "363-U1482A-43F-CC-nan\n",
      "----\n",
      "363-U1482A-44F-CC-nan\n",
      "----\n",
      "363-U1482A-45F-CC-nan\n",
      "----\n",
      "363-U1482A-46X-CC-nan\n",
      "----\n",
      "363-U1482A-47X-CC-nan\n",
      "----\n",
      "363-U1482A-48X-CC-nan\n",
      "----\n",
      "363-U1482A-49X-CC-nan\n",
      "----\n",
      "363-U1482A-50X-CC-nan\n",
      "----\n",
      "363-U1482A-51X-CC-nan\n",
      "----\n",
      "363-U1482A-52X-CC-nan\n",
      "----\n",
      "363-U1482A-53X-CC-nan\n",
      "----\n",
      "363-U1482A-54X-CC-nan\n",
      "----\n",
      "363-U1482A-55X-CC-nan\n",
      "----\n",
      "363-U1482A-56X-CC-nan\n",
      "----\n",
      "363-U1482A-57X-CC-nan\n",
      "----\n",
      "363-U1482A-58X-CC-nan\n"
     ]
    }
   ],
   "source": [
    "for directory in LIMS_paleo_paths:\n",
    "    raw_csvs = glob.glob(f\"{directory}/*.csv\")\n",
    "    for path in raw_csvs:\n",
    "#         print(path)\n",
    "        pass\n",
    "\n",
    "path = 'cleaned_data/Micropal_CSV_1/363-U1482A-Benthic_Forams.csv'\n",
    "content = pd.read_csv(path)\n",
    "content.dropna(inplace=True, axis='index', how='all')\n",
    "my_create_temp_sample_name(content)\n",
    "\n",
    "content['invalid_sample_name'] = content['Sample'] != content['temp_sample']\n",
    "\n",
    "if sum(content['invalid_sample_name']) > 0:\n",
    "    print(path)\n",
    "\n",
    "#             print(content[['Sample', 'temp_sample']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## duplicate sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "files = set()\n",
    "\n",
    "for directory in LIMS_paleo_paths:\n",
    "    raw_csvs = glob.glob(f\"{directory}/*.csv\")\n",
    "    for path in raw_csvs:\n",
    "        cols = ['Sample', 'Top [cm]', 'Top Depth [m]', 'Bottom [cm]', 'Bottom Depth [m]',\n",
    "               'Zone name', 'Zone name (short)', 'Extra Sample ID Data']\n",
    "        df = pd.read_csv(path, usecols = cols)\n",
    "        new_df = df[df.duplicated()]\n",
    "        for index, row in new_df.iterrows():\n",
    "            data.append({'sample': row['Sample'], 'path': path})\n",
    "            files.add(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(data)\n",
    "new_df.to_csv('tmp/csvs_with_duplicate_sample_names.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gather problematic files for PI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_paths(metadata_path, output_base_path):\n",
    "    df = pd.read_csv(metadata_path)\n",
    "    df['raw_data_path'] = ''\n",
    "    raw_data_index = df.columns.get_loc('raw_data_path')\n",
    "    df['output_path'] = ''\n",
    "    output_index = df.columns.get_loc('output_path')\n",
    "    df['relative_path'] = ''\n",
    "    relative_index = df.columns.get_loc('relative_path')\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        parts = row['path'].split('/')\n",
    "        original_directory = parts[1]\n",
    "        filename = parts[2]\n",
    "\n",
    "        if original_directory == 'Micropal_CSV_1':\n",
    "            directory = 'DESC Micropal CSV 1'\n",
    "        elif original_directory == 'Micropal_CSV_2':\n",
    "            directory = 'DESC Micropal CSV 2'\n",
    "        elif original_directory == 'Micropal_CSV_3':\n",
    "            directory = 'DESC Micropal CSV 3'\n",
    "        elif original_directory == 'Micropal_CSV_revised':\n",
    "            directory = 'DESC Micropal CSV revised'\n",
    "        else:\n",
    "            directory = 'DESC-Lithology-CSV'\n",
    "            \n",
    "        \n",
    "        df.iloc[index, raw_data_index]  = f'raw_data/{directory}/{filename}'\n",
    "        df.iloc[index, output_index]  = f'{output_base_path}/{directory}/{filename}'\n",
    "        df.iloc[index, relative_index]  = f'{directory}/{filename}'\n",
    "\n",
    "\n",
    "    df.to_csv(metadata_path, index=False)\n",
    "    \n",
    "def copy_files(metadata_path):\n",
    "    df = pd.read_csv(metadata_path)\n",
    "    \n",
    "    directories = [re.sub('/[A-Za-z0-9\\-_ ]+\\.csv$', '', path)for path in list(df['output_path'])]\n",
    "    unique_directories = set(directories)\n",
    "    for directory in unique_directories:\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    for index, row in df.iterrows():\n",
    "        shutil.copy(row['raw_data_path'], row['output_path'])\n",
    "        \n",
    "\n",
    "def create_sample_name(df):\n",
    "    \"\"\"Uses Exp...A/W columns to create a name for a sample\"\"\"\n",
    "    names = {\"Exp\", \"Site\", \"Hole\", \"Core\", \"Type\", \"Section\", \"A/W\"}\n",
    "    if names.issubset(df.columns):\n",
    "        df[\"Temp_Sample\"] = df.apply(\n",
    "            lambda row: create_sample_name_for_row(row, df.columns), axis=1\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"File does not have the expected columns.\")\n",
    "        \n",
    "\n",
    "def process_files(metadata_path):\n",
    "    df = pd.read_csv(metadata_path)\n",
    "    for index, row in df.iterrows():\n",
    "        content = pd.read_csv(row['output_path'], dtype=str)\n",
    "        \n",
    "        if \"Sample\" in content.columns:\n",
    "            pass\n",
    "        elif \"Label ID\" in content.columns:\n",
    "            pass\n",
    "        else:\n",
    "            content['Temp_Sample'] = ''\n",
    "            create_sample_name(content)\n",
    "            \n",
    "        content = csv_cleanup(content, row['output_path'])\n",
    "        content.to_csv(row['output_path'], index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = 'tmp/csvs_with_duplicate_sample_names.csv'\n",
    "output_base_path = 'tmp/duplicate_samples'\n",
    "\n",
    "add_paths(metadata_path, output_base_path)\n",
    "copy_files(metadata_path)\n",
    "process_files(metadata_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = 'tmp/csvs_with_missing_samples.csv'\n",
    "output_base_path = 'tmp/missing_samples'\n",
    "\n",
    "add_paths(metadata_path, output_base_path)\n",
    "copy_files(metadata_path)\n",
    "process_files(metadata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find all taxon groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxon_groups = set()\n",
    "\n",
    "for directory in LIMS_paleo_paths:\n",
    "    raw_csvs = glob.glob(f\"{directory}/*.csv\")\n",
    "    for path in raw_csvs:\n",
    "        \n",
    "        parts = path.split('/')\n",
    "        filename = parts[2]\n",
    "        partial_path = '/'.join(parts[1:3])\n",
    "        taxon_group = extract_taxon_group_from_filename(filename)\n",
    "        taxon_groups.add(taxon_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'benthic_forams',\n",
       " 'bolboformids',\n",
       " 'chrysophyte_cysts',\n",
       " 'diatoms',\n",
       " 'dinoflagellates',\n",
       " 'ebridians',\n",
       " 'nannofossils',\n",
       " 'ostracods',\n",
       " 'palynology',\n",
       " 'planktic_forams',\n",
       " 'radiolarians',\n",
       " 'silicoflagellates'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxon_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
