{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicate sample names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import glob\n",
    "import psycopg2\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../scripts/')\n",
    "sys.path.append('../')\n",
    "\n",
    "from normalize_data import (\n",
    "    csv_cleanup\n",
    ")\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for duplicate samples in one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53     363-U1482A-14H-CC-PAL-NANNO\n",
       "58     363-U1482A-15H-CC-PAL-NANNO\n",
       "63     363-U1482A-16H-CC-PAL-NANNO\n",
       "68     363-U1482A-17H-CC-PAL-NANNO\n",
       "70     363-U1482A-18H-CC-PAL-nanno\n",
       "72     363-U1482A-19H-CC-PAL-NANNO\n",
       "74     363-U1482A-20H-CC-PAL-NANNO\n",
       "79     363-U1482A-21H-CC-PAL-NANNO\n",
       "80     363-U1482A-21H-CC-PAL-NANNO\n",
       "81     363-U1482A-21H-CC-PAL-NANNO\n",
       "90     363-U1482A-23H-CC-PAL-NANNO\n",
       "92     363-U1482A-24H-CC-PAL-NANNO\n",
       "94     363-U1482A-25H-CC-PAL-NANNO\n",
       "96     363-U1482A-26H-CC-PAL-NANNO\n",
       "98     363-U1482A-27H-CC-PAL-NANNO\n",
       "106    363-U1482A-32H-CC-PAL-NANNO\n",
       "108    363-U1482A-33H-CC-PAL-NANNO\n",
       "110    363-U1482A-34H-CC-PAL-NANNO\n",
       "112    363-U1482A-35H-CC-PAL-NANNO\n",
       "114    363-U1482A-36H-CC-PAL-NANNO\n",
       "116    363-U1482A-37H-CC-PAL-NANNO\n",
       "123    363-U1482A-43F-CC-PAL-NANNO\n",
       "125    363-U1482A-44F-CC-PAL-NANNO\n",
       "127    363-U1482A-45F-CC-PAL-NANNO\n",
       "129    363-U1482A-46X-CC-PAL-NANNO\n",
       "130    363-U1482A-46X-CC-PAL-NANNO\n",
       "131    363-U1482A-46X-CC-PAL-NANNO\n",
       "133    363-U1482A-47X-CC-PAL-NANNO\n",
       "134    363-U1482A-47X-CC-PAL-NANNO\n",
       "136    363-U1482A-48X-CC-PAL-NANNO\n",
       "138    363-U1482A-49X-CC-PAL-NANNO\n",
       "139    363-U1482A-49X-CC-PAL-NANNO\n",
       "141    363-U1482A-50X-CC-PAL-NANNO\n",
       "143    363-U1482A-51X-CC-PAL-NANNO\n",
       "Name: Sample, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'cleaned_data/Micropal_CSV_1/363-U1482A-nannofossils.csv'\n",
    "content = pd.read_csv(path)\n",
    "\n",
    "# cols = ['Sample', 'Top [cm]', 'Bottom [cm]', 'Top Depth [m]','Bottom Depth [m]']\n",
    "dups = content.duplicated(subset=['Sample'])\n",
    "content[dups]['Sample'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for duplicate samples in all mircopal files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_paths = [\n",
    "    'cleaned_data/Micropal_CSV_1', \n",
    "    'cleaned_data/Micropal_CSV_2',\n",
    "    'cleaned_data/Micropal_CSV_3',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =[]\n",
    "for clean_data_path in clean_data_paths:\n",
    "    raw_csvs = glob.glob(f\"{clean_data_path}/*.csv\")\n",
    "\n",
    "    for path in raw_csvs:\n",
    "        content = pd.read_csv(path)\n",
    "        \n",
    "        dups = content.duplicated(subset=['Sample'])\n",
    "        new_df = content[dups][['Sample']] \n",
    "        for index, row in new_df.iterrows():\n",
    "            data.append({'sample': row['Sample'], 'path': path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(data)\n",
    "new_df.to_csv('cleaned_data/csvs_with_duplicate_samples.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import all samples into db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect():\n",
    "    return psycopg2.connect(\n",
    "    host=config.DB_HOST,\n",
    "    database=config.DB_DATABASE_2,\n",
    "    user=config.DB_USER,\n",
    "    password=config.DB_PASSWORD)\n",
    "\n",
    "def clean_row(row):\n",
    "    for key, value in row.items():\n",
    "        if isinstance(value, str):\n",
    "            row[key] = value.strip()\n",
    "        elif value is  None:\n",
    "            row[key] = None\n",
    "        elif value is  np.nan:\n",
    "            row[key] = None\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_paths = [\n",
    "    'cleaned_data/Micropal_CSV_1' ,\n",
    "    'cleaned_data/Micropal_CSV_2',\n",
    "    'cleaned_data/Micropal_CSV_3',\n",
    "    'cleaned_data/Micropal_CSV_revised'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_data/Micropal_CSV_1\n",
      "cleaned_data/Micropal_CSV_2\n",
      "cleaned_data/Micropal_CSV_3\n",
      "cleaned_data/Micropal_CSV_revised\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "conn = connect()\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for clean_data_path in clean_data_paths:\n",
    "    raw_csvs = glob.glob(f\"{clean_data_path}/*.csv\")\n",
    "    print(clean_data_path)\n",
    "\n",
    "    for path in raw_csvs:\n",
    "        filename = path.split('/')[2]\n",
    "        content = pd.read_csv(path, dtype=str)\n",
    "\n",
    "\n",
    "        for index, row in content.iterrows():\n",
    "            row = clean_row(row)\n",
    "            sql = \"\"\"\n",
    "                INSERT INTO samples (name, exp, site, hole, core, type,\n",
    "                section, aw, extra_sample_id, top, bottom, top_depth, bottom_depth, \n",
    "                data_source_notes, zone_name, zone_name_short, created_at) \n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\n",
    "                \"\"\"\n",
    "            cursor.execute(sql, (row['Sample'], row['Exp'], row['Site'], row['Hole'],\n",
    "                                row['Core'], row['Type'], row['Section'], row['A/W'], \n",
    "                                row['Extra Sample ID Data'], row['Top [cm]'], row['Bottom [cm]'],\n",
    "                                row['Top Depth [m]'], row['Bottom Depth [m]'], \n",
    "                                path, row['Zone name'], row['Zone name (short)'],\n",
    "                                datetime.datetime.now()));\n",
    "\n",
    "    conn.commit()\n",
    "conn.close()\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_data/Lithology_CSV\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "conn = connect()\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for clean_data_path in ['cleaned_data/Lithology_CSV']:\n",
    "    raw_csvs = glob.glob(f\"{clean_data_path}/*.csv\")\n",
    "    print(clean_data_path)\n",
    "\n",
    "    for path in raw_csvs:\n",
    "        filename = path.split('/')[2]\n",
    "        content = pd.read_csv(path, dtype=str)\n",
    "\n",
    "        for index, row in content.iterrows():\n",
    "            row = clean_row(row)\n",
    "            sql = \"\"\"\n",
    "                INSERT INTO samples (name, exp, site, hole, core, type,\n",
    "                section, aw, extra_sample_id, top, bottom, top_depth, bottom_depth, \n",
    "                data_source_notes, created_at) \n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\n",
    "                \"\"\"\n",
    "            cursor.execute(sql, (row['Sample'], row['Exp'], row['Site'], row['Hole'],\n",
    "                                row['Core'], row['Type'], row['Section'], row['A/W'], \n",
    "                                row['Extra Sample ID Data'], row['Top [cm]'], row['Bottom [cm]'],\n",
    "                                row['Top Depth [m]'], row['Bottom Depth [m]'], \n",
    "                                path, \n",
    "                                datetime.datetime.now()));\n",
    "\n",
    "    conn.commit()\n",
    "conn.close()\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading CSV\n",
    "\n",
    "test various settings to read csv\n",
    "https://stackoverflow.com/a/47368368\n",
    "https://stackoverflow.com/a/36909497\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top and bottom is int\n",
    "path = 'raw_data/DESC-Lithology-CSV/376_macroscopic_U1527C_2.csv'\n",
    "\n",
    "# top, bottom, top depth is mixture of int and floats\n",
    "path = 'raw_data/DESC-Lithology-CSV/330_sediment_U1373A.csv'\n",
    "\n",
    "# No data this hole\n",
    "path = 'raw_data/DESC-Lithology-CSV/329_sediment_U1369D.csv'\n",
    "\n",
    "# blank colums with no header or data\n",
    "path = 'raw_data/DESC-Lithology-CSV/329_sediment_U1368D.csv'\n",
    "\n",
    "# blank rows with no data \n",
    "path = 'cleaned_data/Micropal_CSV_3/341_planktic_forams_U1417B.csv'\n",
    "\n",
    "# pandas will add extra decimal places\n",
    "path = 'raw_data/DESC-Lithology-CSV/320 Core Description_U1336B.csv'\n",
    "\n",
    "# top and bottom are int or null, top depth and bottom depth are int or float\n",
    "path = 'raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_files = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original and 1 are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. pandas add extra decimal places to floats, and convert integers to floats\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "output = path + '1.csv' if new_files else path\n",
    "df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 and 2 are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. float_precision='round_trip' prevents adding random extra decimal positions\n",
    "\n",
    "df = pd.read_csv(path, float_precision='round_trip')\n",
    "output = path + '2.csv' if new_files else path\n",
    "df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 and 3 are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. na_filter=False prevent converting integer to floats when column has NAs\n",
    "\n",
    "df = pd.read_csv(path, na_filter=False)\n",
    "output = path + '3.csv' if new_files else path\n",
    "df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)\n",
    "df = csv_cleanup(df, path)\n",
    "output = path + '4.csv' if new_files else path\n",
    "df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, float_precision='round_trip')\n",
    "df = csv_cleanup(df, path)\n",
    "output = path + '5.csv' if new_files else path\n",
    "df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, float_precision='round_trip',  na_filter=False)\n",
    "df = csv_cleanup(df, path)\n",
    "output = path + '6.csv' if new_files else path\n",
    "df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set type to strings for columns\n",
    "\n",
    "df = pd.read_csv(path,\n",
    "                 dtype = {'Top [cm]': str, 'Bottom [cm]': str, \n",
    "                         'Top Depth [m]': str, 'Bottom Depth [m]': str})\n",
    "output = path + '7.csv' if new_files else path\n",
    "df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set type to strings for dataframe\n",
    "\n",
    "df = pd.read_csv(path, dtype = str)\n",
    "output = path + '8.csv' if new_files else path\n",
    "df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv1.csv\n",
      "diff notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv1.csv notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv2.csv\n",
      "diff notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv2.csv notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv3.csv\n",
      "diff notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv3.csv notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv4.csv\n",
      "diff notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv4.csv notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv5.csv\n",
      "diff notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv5.csv notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv6.csv\n",
      "diff notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv6.csv notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv7.csv\n",
      "diff notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv7.csv notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv8.csv\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    if i == 0:\n",
    "        print(f'diff notebooks/{path} notebooks/{path}{i + 1}.csv')\n",
    "    else:\n",
    "        print(f'diff notebooks/{path}{i}.csv notebooks/{path}{i + 1}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read and write every LIMS file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology = 'cleaned_data/Lithology_CSV'\n",
    "micropal_1 = 'cleaned_data/Micropal_CSV_1'\n",
    "micropal_2 = 'cleaned_data/Micropal_CSV_2'\n",
    "micropal_3 = 'cleaned_data/Micropal_CSV_3'\n",
    "\n",
    "directories = [lithology, micropal_1, micropal_2, micropal_3]\n",
    "\n",
    "lithology = 'raw_data/DESC-Lithology-CSV'\n",
    "micropal_1 = 'raw_data/DESC Micropal CSV 1'\n",
    "micropal_2 = 'raw_data/DESC Micropal CSV 2'\n",
    "micropal_3 = 'raw_data/DESC Micropal CSV 3'\n",
    "\n",
    "directories = [lithology, micropal_1, micropal_2, micropal_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in directories:\n",
    "    paths = glob.glob(f\"{directory}/*.csv\")\n",
    "\n",
    "    for path in paths:\n",
    "        df = pd.read_csv(path, dtype=str)\n",
    "        df = csv_cleanup(df, path)\n",
    "        df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv'\n",
    "\n",
    "df = pd.read_csv(path, dtype=str)\n",
    "df = csv_cleanup(df, path)\n",
    "df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
