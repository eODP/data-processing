{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicate sample names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "sys.path.append('../scripts/')\n",
    "from normalize_data import (\n",
    "    csv_cleanup\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for duplicate samples in one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53     363-U1482A-14H-CC-PAL-NANNO\n",
       "58     363-U1482A-15H-CC-PAL-NANNO\n",
       "63     363-U1482A-16H-CC-PAL-NANNO\n",
       "68     363-U1482A-17H-CC-PAL-NANNO\n",
       "70     363-U1482A-18H-CC-PAL-nanno\n",
       "72     363-U1482A-19H-CC-PAL-NANNO\n",
       "74     363-U1482A-20H-CC-PAL-NANNO\n",
       "79     363-U1482A-21H-CC-PAL-NANNO\n",
       "80     363-U1482A-21H-CC-PAL-NANNO\n",
       "81     363-U1482A-21H-CC-PAL-NANNO\n",
       "90     363-U1482A-23H-CC-PAL-NANNO\n",
       "92     363-U1482A-24H-CC-PAL-NANNO\n",
       "94     363-U1482A-25H-CC-PAL-NANNO\n",
       "96     363-U1482A-26H-CC-PAL-NANNO\n",
       "98     363-U1482A-27H-CC-PAL-NANNO\n",
       "106    363-U1482A-32H-CC-PAL-NANNO\n",
       "108    363-U1482A-33H-CC-PAL-NANNO\n",
       "110    363-U1482A-34H-CC-PAL-NANNO\n",
       "112    363-U1482A-35H-CC-PAL-NANNO\n",
       "114    363-U1482A-36H-CC-PAL-NANNO\n",
       "116    363-U1482A-37H-CC-PAL-NANNO\n",
       "123    363-U1482A-43F-CC-PAL-NANNO\n",
       "125    363-U1482A-44F-CC-PAL-NANNO\n",
       "127    363-U1482A-45F-CC-PAL-NANNO\n",
       "129    363-U1482A-46X-CC-PAL-NANNO\n",
       "130    363-U1482A-46X-CC-PAL-NANNO\n",
       "131    363-U1482A-46X-CC-PAL-NANNO\n",
       "133    363-U1482A-47X-CC-PAL-NANNO\n",
       "134    363-U1482A-47X-CC-PAL-NANNO\n",
       "136    363-U1482A-48X-CC-PAL-NANNO\n",
       "138    363-U1482A-49X-CC-PAL-NANNO\n",
       "139    363-U1482A-49X-CC-PAL-NANNO\n",
       "141    363-U1482A-50X-CC-PAL-NANNO\n",
       "143    363-U1482A-51X-CC-PAL-NANNO\n",
       "Name: Sample, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'cleaned_data/Micropal_CSV_1/363-U1482A-nannofossils.csv'\n",
    "content = pd.read_csv(path)\n",
    "\n",
    "# cols = ['Sample', 'Top [cm]', 'Bottom [cm]', 'Top Depth [m]','Bottom Depth [m]']\n",
    "dups = content.duplicated(subset=['Sample'])\n",
    "content[dups]['Sample'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for duplicate samples in all mircopal files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_paths = [\n",
    "    'cleaned_data/Micropal_CSV_1', \n",
    "    'cleaned_data/Micropal_CSV_2',\n",
    "    'cleaned_data/Micropal_CSV_3',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =[]\n",
    "for clean_data_path in clean_data_paths:\n",
    "    raw_csvs = glob.glob(f\"{clean_data_path}/*.csv\")\n",
    "\n",
    "    for path in raw_csvs:\n",
    "        content = pd.read_csv(path)\n",
    "        \n",
    "        dups = content.duplicated(subset=['Sample'])\n",
    "        new_df = content[dups][['Sample']] \n",
    "        for index, row in new_df.iterrows():\n",
    "            data.append({'sample': row['Sample'], 'path': path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(data)\n",
    "new_df.to_csv('cleaned_data/csvs_with_duplicate_samples.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import all samples into db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_paths = [\n",
    "    'cleaned_data/Micropal_CSV_1', \n",
    "    'cleaned_data/Micropal_CSV_2',\n",
    "    'cleaned_data/Micropal_CSV_3',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect():\n",
    "    return psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"eodp_dev\",\n",
    "    user=\"wyk\",\n",
    "    password=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1359D_Diatoms_2.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1359D_Diatoms_2.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1359C_Diatoms_2.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/320_U1331C_Radiolarians_3.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/320_U1331C_Radiolarians_3.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1360A_Diatoms_2.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1360A_Diatoms_2.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1360A_Diatoms_2.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1358A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1358A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1358A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1358A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1358A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1358A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1358A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1358A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1358A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1358A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1358A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1358A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1358A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1358A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1358A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1358A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1358A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1358A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1358A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1358B_Diatoms_2.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1356A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1356A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_1/318_U1356A_Diatoms_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_2/350_U1436C_nannofossils.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_2/350_U1436C_nannofossils.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_2/350_U1436C_nannofossils.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_2/350_U1437D_nannofossils.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_2/350_U1437D_nannofossils.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1388B_5.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1388B_5.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1388B_5.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1388B_5.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1388B_4.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1388B_4.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1388B_4.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1388B_4.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1388B_4.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1388B_6.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1388B_6.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1388B_6.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1388B_6.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1386B_6.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1388B_3.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1388B_3.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1388B_3.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1388B_2.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1388B_2.csv\n",
      "nan 0 0 53.0 53.0 cleaned_data/Micropal_CSV_3/342_nannofossils_U1410A_1.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_palynology_U1387C.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_palynology_U1387C.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/330_nannofossils_U1372A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/330_nannofossils_U1372A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/330_nannofossils_U1372A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/330_nannofossils_U1372A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/330_nannofossils_U1372A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1386C_4.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/341_planktic_forams_U1417A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/341_planktic_forams_U1417C.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/341_planktic_forams_U1417C.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/341_planktic_forams_U1417C.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/341_planktic_forams_U1417B.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/341_planktic_forams_U1417B.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/341_planktic_forams_U1417B.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1391A_6.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_nannofossils_U1386A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_nannofossils_U1386A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_nannofossils_U1386A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_nannofossils_U1386A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_nannofossils_U1386A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_nannofossils_U1386A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_nannofossils_U1386A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_nannofossils_U1386A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_nannofossils_U1386A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_nannofossils_U1386A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_nannofossils_U1386A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_nannofossils_U1386A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_nannofossils_U1386A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_nannofossils_U1386A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_nannofossils_U1386A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_nannofossils_U1386A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_nannofossils_U1386A.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1385A_6.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1385A_6.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1385A_6.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1385A_6.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1385A_6.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1385A_6.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1385A_6.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1385A_6.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1389A_5.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1385A_5.csv\n",
      "nan 0 1 243.87 243.88 cleaned_data/Micropal_CSV_3/342_benthic_forams_U1408A_2.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1387C_3.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1387C_3.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1389A_4.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1389A_4.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1387C_6.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1387C_6.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1389A_3.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1389A_3.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/330_nannofossils_U1374A_2.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1387C_5.csv\n",
      "nan nan nan nan nan cleaned_data/Micropal_CSV_3/339_benthic_forams_U1387C_5.csv\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "conn = connect()\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for clean_data_path in clean_data_paths:\n",
    "    raw_csvs = glob.glob(f\"{clean_data_path}/*.csv\")\n",
    "\n",
    "    for path in raw_csvs:\n",
    "        filename = path.split('/')[2]\n",
    "        content = pd.read_csv(path)\n",
    "\n",
    "        for index, row in content.iterrows():\n",
    "             if type(row['Sample']) is str and (type(row['Top [cm]']) is int or type(row['Top [cm]']) is float):\n",
    "\n",
    "                 top =  0 if math.isnan(row['Top [cm]']) else row['Top [cm]']\n",
    "                 sample = row['Sample'].strip()\n",
    "                 sql = f\"INSERT INTO staging.samples (name,top,bottom,top_depth,bottom_depth, created_at, data_source_notes)  VALUES (\\'{sample}\\', {top} , {row['Bottom [cm]']} , {row['Top Depth [m]']} ,{row['Bottom Depth [m]']}, now(), \\'{filename}\\');\"\n",
    "                 cursor.execute(sql);\n",
    "             else:\n",
    "                print(row['Sample'], row['Top [cm]'], row['Bottom [cm]'], row['Top Depth [m]'], row['Bottom Depth [m]'], path )\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading CSV\n",
    "\n",
    "test various settings to read csv\n",
    "https://stackoverflow.com/a/47368368\n",
    "https://stackoverflow.com/a/36909497\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top and bottom is int\n",
    "path = 'raw_data/DESC-Lithology-CSV/376_macroscopic_U1527C_2.csv'\n",
    "\n",
    "# top, bottom, top depth is mixture of int and floats\n",
    "path = 'raw_data/DESC-Lithology-CSV/330_sediment_U1373A.csv'\n",
    "\n",
    "# No data this hole\n",
    "path = 'raw_data/DESC-Lithology-CSV/329_sediment_U1369D.csv'\n",
    "\n",
    "# blank colums with no header or data\n",
    "path = 'raw_data/DESC-Lithology-CSV/329_sediment_U1368D.csv'\n",
    "\n",
    "# blank rows with no data \n",
    "path = 'cleaned_data/Micropal_CSV_3/341_planktic_forams_U1417B.csv'\n",
    "\n",
    "# pandas will add extra decimal places\n",
    "path = 'raw_data/DESC-Lithology-CSV/320 Core Description_U1336B.csv'\n",
    "\n",
    "# top and bottom are int or null, top depth and bottom depth are int or float\n",
    "path = 'raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_files = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original and 1 are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. pandas add extra decimal places to floats, and convert integers to floats\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "output = path + '1.csv' if new_files else path\n",
    "df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 and 2 are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. float_precision='round_trip' prevents adding random extra decimal positions\n",
    "\n",
    "df = pd.read_csv(path, float_precision='round_trip')\n",
    "output = path + '2.csv' if new_files else path\n",
    "df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 and 3 are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. na_filter=False prevent converting integer to floats when column has NAs\n",
    "\n",
    "df = pd.read_csv(path, na_filter=False)\n",
    "output = path + '3.csv' if new_files else path\n",
    "df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)\n",
    "df = csv_cleanup(df, path)\n",
    "output = path + '4.csv' if new_files else path\n",
    "df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, float_precision='round_trip')\n",
    "df = csv_cleanup(df, path)\n",
    "output = path + '5.csv' if new_files else path\n",
    "df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, float_precision='round_trip',  na_filter=False)\n",
    "df = csv_cleanup(df, path)\n",
    "output = path + '6.csv' if new_files else path\n",
    "df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set type to strings for columns\n",
    "\n",
    "df = pd.read_csv(path,\n",
    "                 dtype = {'Top [cm]': str, 'Bottom [cm]': str, \n",
    "                         'Top Depth [m]': str, 'Bottom Depth [m]': str})\n",
    "output = path + '7.csv' if new_files else path\n",
    "df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set type to strings for dataframe\n",
    "\n",
    "df = pd.read_csv(path, dtype = str)\n",
    "output = path + '8.csv' if new_files else path\n",
    "df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv1.csv\n",
      "diff notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv1.csv notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv2.csv\n",
      "diff notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv2.csv notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv3.csv\n",
      "diff notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv3.csv notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv4.csv\n",
      "diff notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv4.csv notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv5.csv\n",
      "diff notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv5.csv notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv6.csv\n",
      "diff notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv6.csv notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv7.csv\n",
      "diff notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv7.csv notebooks/raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv8.csv\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    if i == 0:\n",
    "        print(f'diff notebooks/{path} notebooks/{path}{i + 1}.csv')\n",
    "    else:\n",
    "        print(f'diff notebooks/{path}{i}.csv notebooks/{path}{i + 1}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read and write every LIMS file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology = 'cleaned_data/Lithology_CSV'\n",
    "micropal_1 = 'cleaned_data/Micropal_CSV_1'\n",
    "micropal_2 = 'cleaned_data/Micropal_CSV_2'\n",
    "micropal_3 = 'cleaned_data/Micropal_CSV_3'\n",
    "\n",
    "directories = [lithology, micropal_1, micropal_2, micropal_3]\n",
    "\n",
    "lithology = 'raw_data/DESC-Lithology-CSV'\n",
    "micropal_1 = 'raw_data/DESC Micropal CSV 1'\n",
    "micropal_2 = 'raw_data/DESC Micropal CSV 2'\n",
    "micropal_3 = 'raw_data/DESC Micropal CSV 3'\n",
    "\n",
    "directories = [lithology, micropal_1, micropal_2, micropal_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in directories:\n",
    "    paths = glob.glob(f\"{directory}/*.csv\")\n",
    "\n",
    "    for path in paths:\n",
    "        df = pd.read_csv(path, dtype=str)\n",
    "        df = csv_cleanup(df, path)\n",
    "        df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'raw_data/DESC-Lithology-CSV/342_sediment_U1406C.csv'\n",
    "\n",
    "df = pd.read_csv(path, dtype=str)\n",
    "df = csv_cleanup(df, path)\n",
    "df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
