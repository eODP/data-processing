{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA ODP files - didn't use the files; \n",
    "\n",
    "- the tar.gz paleo files are the same paleo files as Morgan web scrapped;  \n",
    "- the web scrapped files also have age models; \n",
    "- these files are txt; bad_tabs 1, bad_encoding 6, space_delim 61\n",
    "- the web scrapped are csv; bad_tabs 0, bad_encoding 0, space_delim 61\n",
    "\n",
    "expeditions 101 - 210; range_tables for paleo fossils; https://www.ngdc.noaa.gov/mgg/geology/data/joides_resolution/odp_all_paleontology.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "sys.path.append('../scripts/')\n",
    "import glob\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import space_delim as sd\n",
    "import normalize_noaa_files as nf\n",
    "\n",
    "from normalize_noaa_files import (\n",
    "    unique_filenames_for_paths,\n",
    "    unique_columns_for_paths,\n",
    "    qa_files_for_paths,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = 'cleaned_data'\n",
    "base_directory = 'raw_data'\n",
    "\n",
    "base_data_path = os.path.join(base_directory, 'odp_all_paleontology', 'range_tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files 2045\n"
     ]
    }
   ],
   "source": [
    "csv_paths = glob.glob(os.path.join(base_data_path,'**' ,'*.txt'), recursive=True)\n",
    "print('files', len(csv_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unique file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Benthic Foraminifers.txt',\n",
       " 'Benthic_Foraminifers.txt',\n",
       " 'Bolboforms.txt',\n",
       " 'Diatoms.txt',\n",
       " 'Dinoflagellates_Acritarch_Prasinophytes.txt',\n",
       " 'Dinoflagellates_Acritarchs_Prasinophytes.txt',\n",
       " 'Macrofossils.txt',\n",
       " 'Miscellaneous.txt',\n",
       " 'Nannofossils .txt',\n",
       " 'Nannofossils.txt',\n",
       " 'Ostracodes.txt',\n",
       " 'Planktonic Foraminifers.txt',\n",
       " 'Planktonic_Foraminifers .txt',\n",
       " 'Planktonic_Foraminifers.txt',\n",
       " 'Pollen_Spores.txt',\n",
       " 'Pteropods.txt',\n",
       " 'Radiolarians.txt',\n",
       " 'Silicoflagellates_Ebridians_Actiniscidians.txt',\n",
       " 'Sponge_Spicules.txt',\n",
       " 'Trace_Fossils.txt'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = unique_filenames_for_paths(csv_paths)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare odp vs iodp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### odp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "odp_base_data_path = os.path.join('cleaned_data', 'odp_all_paleontology', 'range_tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files 83\n"
     ]
    }
   ],
   "source": [
    "odp_directories_paths = glob.glob(os.path.join(odp_base_data_path,'**'))\n",
    "print('files', len(odp_directories_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files 2045\n"
     ]
    }
   ],
   "source": [
    "odp_csv_paths = glob.glob(os.path.join(odp_base_data_path,'**' ,'*.txt'), recursive=True)\n",
    "print('files', len(odp_csv_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odp_files = {'Benthic Foraminifers.txt',\n",
    " 'Benthic_Foraminifers.txt',\n",
    " 'Bolboforms.txt',\n",
    " 'Diatoms.txt',\n",
    " 'Dinoflagellates_Acritarch_Prasinophytes.txt',\n",
    " 'Dinoflagellates_Acritarchs_Prasinophytes.txt',\n",
    " 'Macrofossils.txt',\n",
    " 'Miscellaneous.txt',\n",
    " 'Nannofossils .txt',\n",
    " 'Nannofossils.txt',\n",
    " 'Ostracodes.txt',\n",
    " 'Planktonic Foraminifers.txt',\n",
    " 'Planktonic_Foraminifers .txt',\n",
    " 'Planktonic_Foraminifers.txt',\n",
    " 'Pollen_Spores.txt',\n",
    " 'Pteropods.txt',\n",
    " 'Radiolarians.txt',\n",
    " 'Silicoflagellates_Ebridians_Actiniscidians.txt',\n",
    " 'Sponge_Spicules.txt',\n",
    " 'Trace_Fossils.txt'}\n",
    "\n",
    "len(odp_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odp_names = {file.replace('.txt', '') for file in odp_files}\n",
    "len(odp_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### janus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "janus_base_data_path = os.path.join('cleaned_data', 'NOAA_csv', 'JanusIODP_paleo_agemodel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cleaned_data/NOAA_csv/JanusIODP_paleo_agemodel/paleontology/range_tables/135', 'cleaned_data/NOAA_csv/JanusIODP_paleo_agemodel/paleontology/range_tables/132', 'cleaned_data/NOAA_csv/JanusIODP_paleo_agemodel/paleontology/range_tables/104']\n",
      "files 83\n"
     ]
    }
   ],
   "source": [
    "janus_directories_paths = glob.glob(os.path.join(janus_base_data_path, 'paleontology', 'range_tables', '**'))\n",
    "print(janus_directories_paths[0:3])\n",
    "print('files', len(janus_directories_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files 2045\n"
     ]
    }
   ],
   "source": [
    "janus_csv_paths = glob.glob(os.path.join(janus_base_data_path, 'paleontology', 'range_tables', '**', '*.csv'), recursive=True)\n",
    "print('files', len(janus_csv_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "janus_files = {'Benthic Foraminifers.csv',\n",
    " 'Benthic_Foraminifers.csv',\n",
    " 'Bolboforms.csv',\n",
    " 'Diatoms.csv',\n",
    " 'Dinoflagellates_Acritarch_Prasinophytes.csv',\n",
    " 'Dinoflagellates_Acritarchs_Prasinophytes.csv',\n",
    " 'Macrofossils.csv',\n",
    " 'Miscellaneous.csv',\n",
    " 'Nannofossils .csv',\n",
    " 'Nannofossils.csv',\n",
    " 'Ostracodes.csv',\n",
    " 'Planktonic Foraminifers.csv',\n",
    " 'Planktonic_Foraminifers .csv',\n",
    " 'Planktonic_Foraminifers.csv',\n",
    " 'Pollen_Spores.csv',\n",
    " 'Pteropods.csv',\n",
    " 'Radiolarians.csv',\n",
    " 'Silicoflagellates_Ebridians_Actiniscidians.csv',\n",
    " 'Sponge_Spicules.csv',\n",
    " 'Trace_Fossils.csv'}\n",
    "\n",
    "len(janus_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "janus_names = {file.replace('.csv', '') for file in janus_files}\n",
    "len(janus_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "janus_names - odp_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12975"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxa_columns = unique_columns_for_paths(csv_paths, sep=\"\\t\")\n",
    "len(taxa_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA ODP paleo files\n",
    "\n",
    "count the number of good files vs files that need to be fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_fields = {\n",
    "    'Data',\n",
    "    'Age From (oldest)',\n",
    "    'Age To (youngest)',\n",
    "    'Zone From (bottom)',\n",
    "    'Zone To  (top)',\n",
    "    'Leg',\n",
    "    'Site',\n",
    "    'H',\n",
    "    'Cor',\n",
    "    'T',\n",
    "    'Sc',\n",
    "    'Top(cm)',\n",
    "    'Depth (mbsf)',\n",
    "    'Scientist',\n",
    "#     'Fossil Group',\n",
    "    'Comment', \n",
    "    'Group Abundance',\n",
    "    'Group Preservation'\n",
    "}\n",
    "\n",
    "results = nf.qa_files_for_paths(csv_paths, expected_fields, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad_tabs 1\n",
      "bad_encoding 6\n",
      "space_delim 61\n",
      "missing_fields 0\n",
      "good_files 1977\n"
     ]
    }
   ],
   "source": [
    "print('bad_tabs', len(results['bad_tabs']))\n",
    "print('bad_encoding', len(results['bad_encoding']))\n",
    "print('space_delim', len(results['space_delim']))\n",
    "print('missing_fields', len(results['missing_fields']))\n",
    "print('good_files', len(results['good_files']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- bad_tabs 1\n",
    "- bad_encoding 6\n",
    "- space_delim 61\n",
    "- missing_fields 0\n",
    "- good_files 1977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['missing_fields']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process latin_encoding\n",
    "handle files with encoding that isn't utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in results['bad_encoding']:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert file to utf-8 encoding\n",
    "https://codereview.stackexchange.com/a/202985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in results['bad_encoding']:\n",
    "    with open(file, 'rb') as f:\n",
    "        content_bytes = f.read()\n",
    "    detected = chardet.detect(content_bytes)\n",
    "    encoding = detected['encoding']\n",
    "    content_text = content_bytes.decode(encoding)\n",
    "    \n",
    "    with open(file, 'w', encoding='utf-8') as f:\n",
    "        f.write(content_text)\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process bad_tabs\n",
    "handle files where the hearers and rows have different number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in results['bad_tabs']:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process space_delim\n",
    "\n",
    "handle files that use random number of spaces to separate the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in results['space_delim']:\n",
    "    file_size = os.path.getsize(file)\n",
    "    print(f'\"{file}\",')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/19759423/convert-a-space-delimited-file-to-comma-separated-values-file-in-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process missing_fields\n",
    "\n",
    "handle files don't have the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in results['missing_fields']:\n",
    "    print(file)\n",
    "    df = pd.read_csv(file, nrows=1)\n",
    "    print(expected_fields - set(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check space_delim files were correctly fixed\n",
    "\n",
    "After converting space delimited files, check the  files for errors. Errors\n",
    "include values that have spaces or columns that have no values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_with_spaces = []\n",
    "fixed_space_delim_files = (\n",
    "    sd.space_delim_files_janus_iodp_1\n",
    "    + sd.space_delim_files_janus_iodp_2\n",
    "    + sd.space_delim_files_janus_iodp_3\n",
    ")\n",
    "\n",
    "skip_fields = {\n",
    "    'Data', 'Age From (oldest)', 'Age To (youngest)', 'Zone From (bottom)', \n",
    "    'Zone To  (top)', 'Leg', 'Site','H', 'Cor', 'T', 'Sc', 'Top(cm)', \n",
    "    'Depth (mbsf)', 'Scientist', 'Comment', 'Fossil Group'\n",
    "}\n",
    "\n",
    "def valid_values(x):\n",
    "    return isinstance(x, str) and ' ' in x\n",
    "\n",
    "for file in fixed_space_delim_files:\n",
    "    filename = os.path.join(base_directory, file)\n",
    "\n",
    "    df = pd.read_csv(filename, dtype=str)\n",
    "    df.dropna(axis=\"columns\", how=\"all\")\n",
    "    \n",
    "    taxa_columns = set(df.columns) - skip_fields\n",
    "    for col in taxa_columns:\n",
    "        # check if there are values with spaces         \n",
    "        if sum(df[col].apply(valid_values)) > 0:\n",
    "            files_with_spaces.append(filename)\n",
    "            print(f'{col}: has space')\n",
    "            print(filename)\n",
    "            print('---')\n",
    "            \n",
    "        # check if column is blank       \n",
    "        if df[col].isnull().values.all():\n",
    "            print(f'{col}: has no values')\n",
    "            print(filename)\n",
    "            print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strip the beginning and ending spaces for every column in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files_with_spaces:\n",
    "    update = False\n",
    "    filename = os.path.join(base_directory, file)\n",
    "\n",
    "    df = pd.read_csv(filename, dtype=str, sep=\"\\t\")\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if sum(df[col].apply(valid_values)) > 0:\n",
    "            df = df.apply(lambda x: x.str.strip())\n",
    "            update = True\n",
    "        \n",
    "    if update:\n",
    "        df.to_csv(filename, index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
